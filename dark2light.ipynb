{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8284600,"sourceType":"datasetVersion","datasetId":4920597}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.transforms import Compose, Resize, Normalize\nfrom os.path import join\nfrom torchvision.models import vgg16\nfrom skimage.metrics import structural_similarity\nfrom skimage.metrics import peak_signal_noise_ratio\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:40:58.284274Z","iopub.execute_input":"2024-05-02T04:40:58.284546Z","iopub.status.idle":"2024-05-02T04:41:02.021051Z","shell.execute_reply.started":"2024-05-02T04:40:58.284520Z","shell.execute_reply":"2024-05-02T04:41:02.020251Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nclass LoLDataset(Dataset):\n    def __init__(self, low_light_dir, well_lit_dir, transform=None):\n        self.low_light_dir = low_light_dir\n        self.well_lit_dir = well_lit_dir\n        self.transform = transform\n        self.image_pairs = []\n\n        # Collect pairs of low-light and well-lit images\n        for root, _, files in os.walk(low_light_dir):\n            for file in files:\n                if file.endswith('.jpg') or file.endswith('.png'):\n                    input_path = os.path.join(root, file)\n                    target_path = os.path.join(well_lit_dir, file)  # Directly use the same file name for well-lit images\n                    if os.path.exists(target_path):\n                        self.image_pairs.append((input_path, target_path))\n\n    def __len__(self):\n        return len(self.image_pairs)\n\n    def __getitem__(self, idx):\n        input_path, target_path = self.image_pairs[idx]\n        try:\n            input_image = Image.open(input_path).convert('RGB')\n            target_image = Image.open(target_path).convert('RGB')\n        except:\n            print(f\"Error opening images: {input_path}, {target_path}\")\n            return None, None\n\n        if self.transform:\n            input_image = self.transform(input_image)\n            target_image = self.transform(target_image)\n\n        return input_image, target_image\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Create the dataset and data loaders\nlow_light_dir = \"/kaggle/input/lol-dataset/GAN/low\"\nwell_lit_dir = \"/kaggle/input/lol-dataset/GAN/high\"\n\n# Check if the directories exist\nif not os.path.exists(low_light_dir):\n    print(f\"The directory {low_light_dir} does not exist.\")\nif not os.path.exists(well_lit_dir):\n    print(f\"The directory {well_lit_dir} does not exist.\")\n\ndataset = LoLDataset(low_light_dir, well_lit_dir, transform=transform)\n\n# Check if the dataset is not empty\nif len(dataset) > 0:\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n    batch_size = 32  # Define your batch size here\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\nelse:\n    print(\"The dataset is empty. Please check the provided directories.\")\n    train_loader = None\n    val_loader = None","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:41:02.022443Z","iopub.execute_input":"2024-05-02T04:41:02.022872Z","iopub.status.idle":"2024-05-02T04:41:03.005665Z","shell.execute_reply.started":"2024-05-02T04:41:02.022843Z","shell.execute_reply":"2024-05-02T04:41:03.004809Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\n# FILEPATH: /d:/College/Sixth Sem/Projects/GAN/Untitled.ipynb\n# check if dataset has images\nif len(dataset) > 0:\n    # Print sample pairs of loaded images\n    sample_pairs = random.sample(dataset.image_pairs, k=5)  # Change the value of k to the desired number of sample pairs\n    for input_path, target_path in sample_pairs:\n        print(f\"Input Image: {input_path}\")\n        print(f\"Target Image: {target_path}\")\n        print()\nelse:\n    print(\"Dataset does not have any images.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:41:03.006889Z","iopub.execute_input":"2024-05-02T04:41:03.007221Z","iopub.status.idle":"2024-05-02T04:41:03.013329Z","shell.execute_reply.started":"2024-05-02T04:41:03.007195Z","shell.execute_reply":"2024-05-02T04:41:03.012441Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Input Image: /kaggle/input/lol-dataset/GAN/low/471.png\nTarget Image: /kaggle/input/lol-dataset/GAN/high/471.png\n\nInput Image: /kaggle/input/lol-dataset/GAN/low/693.png\nTarget Image: /kaggle/input/lol-dataset/GAN/high/693.png\n\nInput Image: /kaggle/input/lol-dataset/GAN/low/634.png\nTarget Image: /kaggle/input/lol-dataset/GAN/high/634.png\n\nInput Image: /kaggle/input/lol-dataset/GAN/low/642.png\nTarget Image: /kaggle/input/lol-dataset/GAN/high/642.png\n\nInput Image: /kaggle/input/lol-dataset/GAN/low/227.png\nTarget Image: /kaggle/input/lol-dataset/GAN/high/227.png\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\n# Define the generator architecture\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super(Generator, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# Define the discriminator architecture\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=6):\n        super(Discriminator, self).__init__()\n\n        # Modification: Add more layers to increase model capacity\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1, padding_mode='reflect'),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1, padding_mode='reflect'),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, y):\n        input = torch.cat([x, y], dim=1)\n        return self.model(input)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:41:03.016379Z","iopub.execute_input":"2024-05-02T04:41:03.016728Z","iopub.status.idle":"2024-05-02T04:41:03.034356Z","shell.execute_reply.started":"2024-05-02T04:41:03.016695Z","shell.execute_reply":"2024-05-02T04:41:03.033436Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Initialize the generator and discriminator\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n# Move models to GPU if available\n# Set device to CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Move models to the device\ngenerator = generator.to(device)\ndiscriminator = discriminator.to(device)\n\n# Check if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    generator = nn.DataParallel(generator)\n    discriminator = nn.DataParallel(discriminator)\nelse:\n    print(\"Using a single GPU or CPU\")\n# Loss functions\nadversarial_loss = nn.BCELoss()\nl1_loss = nn.L1Loss()\n\n# Learning rates\nlr = 0.0001\n\n# Optimizers\ng_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\nd_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# Number of epochs\nnum_epochs = 100\n\n# Lambda for L1 loss\nlambda_l1 = 100\n\nclass PairedImageFolder(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.file_names = os.listdir(join(root_dir, 'low'))\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, idx):\n        img_name = self.file_names[idx]\n        low_image = Image.open(join(self.root_dir, 'low', img_name))\n        high_image = Image.open(join(self.root_dir, 'high', img_name))\n\n        if self.transform:\n            low_image = self.transform(low_image)\n            high_image = self.transform(high_image)\n\n        return low_image, high_image\n\n# Data loaders\ntransform = Compose([Resize((256, 256)), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_dataset = PairedImageFolder('/kaggle/input/lol-dataset/GAN/lol_dataset/our485', transform=transform)\nval_dataset = PairedImageFolder('/kaggle/input/lol-dataset/GAN/lol_dataset/eval15', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:41:03.035379Z","iopub.execute_input":"2024-05-02T04:41:03.035898Z","iopub.status.idle":"2024-05-02T04:41:03.341018Z","shell.execute_reply.started":"2024-05-02T04:41:03.035873Z","shell.execute_reply":"2024-05-02T04:41:03.340030Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"code","source":"ssim_values = []\nepoch_numbers = []\n# Training loop\nfor epoch in range(num_epochs):\n    for input_images, target_images in train_loader:\n        input_images = input_images.to(device)\n        target_images = target_images.to(device)\n\n        # Train the discriminator\n        d_optimizer.zero_grad()\n\n        # Generate fake images\n        fake_images = generator(input_images)\n\n        # Compute discriminator loss on real and fake images\n        real_output = discriminator(input_images, target_images)\n        fake_output = discriminator(input_images, fake_images.detach())\n\n        real_loss = adversarial_loss(real_output, torch.ones_like(real_output))\n        fake_loss = adversarial_loss(fake_output, torch.zeros_like(fake_output))\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Train the generator\n        g_optimizer.zero_grad()\n\n        # Generate fake images\n        fake_images = generator(input_images)\n\n        # Compute generator loss\n        fake_output = discriminator(input_images, fake_images)\n        g_adversarial_loss = adversarial_loss(fake_output, torch.ones_like(fake_output))\n        g_l1_loss = l1_loss(fake_images, target_images)\n        g_loss = g_adversarial_loss + lambda_l1 * g_l1_loss\n\n        g_loss.backward()\n        g_optimizer.step()\n\n    # Evaluate on the validation set\n    generator.eval()\n    discriminator.eval()\n    val_ssim_scores = []\n    val_psnr_scores = []\n    generated_images_all = []\n    for input_images, target_images in val_loader:\n        input_images = input_images.to(device)\n        target_images = target_images.to(device)\n\n        with torch.no_grad():\n            generated_images = generator(input_images)\n            generated_images_np = generated_images.permute(0, 2, 3, 1).detach().cpu().numpy().astype(np.float32)\n            target_images_np = target_images.permute(0, 2, 3, 1).detach().cpu().numpy().astype(np.float32)\n            ssim = structural_similarity(generated_images_np,\n                             target_images_np,\n                             multichannel=True, win_size=3, data_range=1)\n            val_ssim_scores.append(ssim)\n            \n            #psnr\n            psnr = 0.0\n            for i in range(generated_images_np.shape[0]):\n                psnr += peak_signal_noise_ratio(target_images_np[i], generated_images_np[i])\n            val_psnr_scores.append(psnr / generated_images_np.shape[0])\n            \n            \n    # val_avg_ssim = sum(val_ssim_scores) / len(val_ssim_scores)\n    # print(f'Epoch [{epoch+1}/{num_epochs}], Validation SSIM: {val_avg_ssim:.4f}')\n\n    # generator.train()\n    # discriminator.train()\n    val_avg_ssim = sum(val_ssim_scores) / len(val_ssim_scores)\n    val_avg_psnr = sum(val_psnr_scores) / len(val_psnr_scores)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Validation SSIM: {val_avg_ssim:.4f}, Validation PSNR: {val_avg_psnr:.4f}')\n\n#     # Store the SSIM value and epoch number for plotting\n#     ssim_values.append(val_avg_ssim)\n#     epoch_numbers.append(epoch + 1)\n\n#     generator.train()\n#     discriminator.train()\n\nnum_images = len(generated_images_all)\nnrows = int(np.ceil(np.sqrt(num_images)))\nncols = int(np.ceil(num_images / nrows))\nfig, axes = plt.subplots(nrows, ncols, figsize=(15, 15))\n\nfor i, ax in enumerate(axes.flat):\n    if i < num_images:\n        image = generated_images_all[i].permute(1, 2, 0).detach().cpu().numpy()\n        image = (image * 0.5 + 0.5) * 255  # Rescale to [0, 255]\n        image = image.astype(np.uint8)\n        ax.imshow(image)\n        ax.axis('off')\n    else:\n        ax.axis('off')\nplt.tight_layout()\nplt.show()\n\ngenerator.train()\ndiscriminator.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:41:03.342688Z","iopub.execute_input":"2024-05-02T04:41:03.343033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(epoch_numbers, ssim_values)\nplt.xlabel('Epochs')\nplt.ylabel('SSIM')\nplt.title('Structural Similarity Index (SSIM) Curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the trained generator for inference\ngenerator.load_state_dict(torch.load('generator.pth'))\ngenerator.eval()\n\n# Transform for input image\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load and preprocess some sample low-light images\nsample_low_light_images = [Image.open('file:///D:/College/Sixth%20Sem/Projects/GAN/low-lit.jpg') for i in range(5)]\nsample_input_tensors = [transform(img).unsqueeze(0).to(device) for img in sample_low_light_images]\n\n# Generate and display sample relighted images\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor i, input_tensor in enumerate(sample_input_tensors):\n    with torch.no_grad():\n        relighted_image = generator(input_tensor)[0]\n\n    relighted_image = relighted_image.permute(1, 2, 0).detach().cpu().numpy()\n    relighted_image = (relighted_image * 0.5 + 0.5) * 255\n    relighted_image = relighted_image.astype(np.uint8)\n\n    axes[0, i].imshow(sample_low_light_images[i])\n    axes[0, i].set_title('Low-light Input')\n    axes[0, i].axis('off')\n\n    axes[1, i].imshow(relighted_image)\n    axes[1, i].set_title('Relighted Output')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.savefig('/kaggle/input/lol-dataset/GAN/relighted_images.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}